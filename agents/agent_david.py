#!/usr/bin/env python3
"""
Agent David - Especialista em limpeza de √°udio
Remove pausas, gagueiras, v√≠cios de linguagem e melhora qualidade do √°udio
"""

import os
import json
import subprocess
import time
import requests
from pathlib import Path
from typing import Dict, List, Any, Optional
from base_agent import BaseAgent, DecisionTypes, AgentStatus

# Carregar vari√°veis do .env
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    # Se dotenv n√£o estiver instalado, tentar carregar manualmente
    env_file = Path('.env')
    if env_file.exists():
        with open(env_file) as f:
            for line in f:
                if '=' in line and not line.startswith('#'):
                    key, value = line.strip().split('=', 1)
                    os.environ[key] = value

class AgentDavid(BaseAgent):
    """
    Agent David - Especialista em Audio Cleaning
    
    MISS√ÉO: Limpar e otimizar √°udio dos chunks para criar conte√∫do din√¢mico
    Remove pausas desnecess√°rias, gagueiras, v√≠cios de linguagem ("hmm", "ahh", "tipo")
    """
    
    def __init__(self):
        super().__init__("DAVID", "AUDIO_CLEANING_SPECIALIST")
        
        # Configura√ß√µes espec√≠ficas do David
        self.temp_dir = self.agent_dir / 'temp_audio'
        self.temp_dir.mkdir(exist_ok=True)
        
        # Configura√ß√µes de limpeza de √°udio
        self.silence_threshold = -40  # dB para detectar sil√™ncio
        self.min_silence_duration = 0.5  # segundos m√≠nimos para considerar pausa
        self.filler_words = ['hmm', 'ahh', 'uhh', 'tipo', 'n√©', 'ent√£o', 'assim']
        
        print(f"üîß Configura√ß√µes de √°udio:")
        print(f"   Threshold sil√™ncio: {self.silence_threshold} dB")
        print(f"   Dura√ß√£o m√≠n. pausa: {self.min_silence_duration}s")
        print(f"   Palavras-filtro: {len(self.filler_words)} configuradas")
    
    def execute(self) -> bool:
        """
        OBRIGAT√ìRIO: M√©todo principal de execu√ß√£o do Agent David.
        Implementa o pipeline completo de limpeza de √°udio com IA.
        """
        print(f"\n{'='*60}")
        print(f"üéµü§ñ {self.name}: INICIANDO LIMPEZA DE √ÅUDIO COM IA")
        print(f"{'='*60}")
        
        # Verificar API key
        if not os.getenv('GEMINI_API_KEY'):
            self.log_processing_step("API_CHECK", AgentStatus.ERROR, "GEMINI_API_KEY n√£o encontrada")
            return False
        
        # Carregar plano do Coordinator
        processing_plan = self.load_processing_plan()
        if not processing_plan:
            return False
        
        self.log_processing_step("PLAN_LOADED", AgentStatus.SUCCESS, 
                               f"Tipo: {processing_plan['strategy']['target_content_type']}")
        
        # Processar chunks com IA
        results = self.process_chunks_with_ai(processing_plan)
        
        # Salvar resultados consolidados
        results_file = self.save_results(results)
        
        # Gerar feedback para o Coordinator
        self._generate_david_feedback(results, processing_plan)
        
        return results['status'] == 'COMPLETED'
    
    def load_processing_plan(self) -> Optional[Dict[str, Any]]:
        """Carrega plano de processamento criado pelo Coordinator"""
        print(f"\nüìã {self.name}: Carregando plano de processamento...")
        
        plan_file = self.coordinator_dir / 'processing_plan.json'
        
        if not plan_file.exists():
            print(f"‚ùå Plano n√£o encontrado: {plan_file}")
            return None
        
        try:
            with open(plan_file, 'r', encoding='utf-8') as f:
                plan = json.load(f)
            
            # Verificar se √© tarefa para David
            david_instructions = plan['strategy']['agent_instructions'].get('DAVID')
            if not david_instructions:
                print(f"‚ùå Nenhuma instru√ß√£o para {self.name} no plano")
                return None
            
            print(f"‚úÖ Plano carregado com instru√ß√µes para {self.name}")
            return plan
            
        except Exception as e:
            print(f"‚ùå Erro ao carregar plano: {e}")
            return None
    
    def analyze_chunk_with_ai(self, chunk_path: str, instructions: Dict[str, Any]) -> Dict[str, Any]:
        """Usa IA para analisar chunk e criar estrat√©gia de limpeza personalizada"""
        print(f"üß† {self.name}: Analisando chunk com IA - {Path(chunk_path).name}")
        
        try:
            # Primeiro, extrair informa√ß√µes t√©cnicas b√°sicas
            technical_analysis = self._get_technical_analysis(chunk_path)
            
            # Usar IA para criar estrat√©gia baseada nas instru√ß√µes
            ai_strategy = self._create_cleaning_strategy_with_ai(chunk_path, technical_analysis, instructions)
            
            return {
                'chunk_path': chunk_path,
                'technical_analysis': technical_analysis,
                'ai_strategy': ai_strategy,
                'needs_cleaning': ai_strategy.get('requires_processing', True)
            }
            
        except Exception as e:
            print(f"   ‚ùå Erro na an√°lise IA: {e}")
            return {'chunk_path': chunk_path, 'needs_cleaning': False, 'error': str(e)}
    
    def _get_technical_analysis(self, chunk_path: str) -> Dict[str, Any]:
        """Extrai informa√ß√µes t√©cnicas E transcri√ß√£o do chunk"""
        try:
            # Informa√ß√µes de √°udio com ffprobe
            cmd = [
                'ffprobe', '-v', 'quiet', '-print_format', 'json',
                '-show_streams', '-show_format', chunk_path
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            media_info = json.loads(result.stdout)
            
            # Detectar sil√™ncios (mant√©m por compatibilidade)
            silence_cmd = [
                'ffmpeg', '-i', chunk_path, '-af', 
                f'silencedetect=noise={self.silence_threshold}dB:d={self.min_silence_duration}',
                '-f', 'null', '-'
            ]
            
            silence_result = subprocess.run(silence_cmd, capture_output=True, text=True)
            silence_periods = self._parse_silence_detection(silence_result.stderr)
            
            # Analisar volume m√©dio
            volume_cmd = [
                'ffmpeg', '-i', chunk_path, '-af', 'volumedetect',
                '-f', 'null', '-'
            ]
            
            volume_result = subprocess.run(volume_cmd, capture_output=True, text=True)
            volume_info = self._parse_volume_analysis(volume_result.stderr)
            
            # NOVA: Extrair transcri√ß√£o do √°udio
            print(f"   üé§ Transcrevendo √°udio para detectar v√≠cios...")
            transcription = self._transcribe_audio_whisper_api(chunk_path)
            
            return {
                'duration': float(media_info.get('format', {}).get('duration', 0)),
                'silence_periods': silence_periods,
                'total_silence_duration': sum(p['duration'] for p in silence_periods),
                'silence_count': len(silence_periods),
                'volume_info': volume_info,
                'audio_streams': len([s for s in media_info.get('streams', []) if s.get('codec_type') == 'audio']),
                'transcription': transcription  # NOVO!
            }
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erro an√°lise t√©cnica: {e}")
            return {'duration': 0, 'silence_periods': [], 'transcription': {'text': '', 'segments': []}, 'error': str(e)}
    
    def _transcribe_audio_whisper_api(self, chunk_path: str) -> Dict[str, Any]:
        """Transcreve √°udio usando OpenAI Whisper API para detectar v√≠cios"""
        try:
            # Extrair √°udio do v√≠deo primeiro
            audio_path = self.temp_dir / f"{Path(chunk_path).stem}_audio.wav"
            
            cmd = [
                'ffmpeg', '-i', chunk_path,
                '-vn', '-acodec', 'pcm_s16le',
                '-ar', '16000', '-ac', '1',
                '-y', str(audio_path)
            ]
            
            subprocess.run(cmd, capture_output=True, check=True)
            
            if not audio_path.exists():
                return {'text': '', 'segments': []}
            
            # Usar Whisper API da OpenAI
            try:
                import openai
            except ImportError:
                print(f"   ‚ö†Ô∏è openai library n√£o instalada: pip install openai")
                return {'text': '', 'segments': []}
            
            openai_api_key = os.getenv('OPENAI_API_KEY')
            if not openai_api_key:
                print(f"   ‚ö†Ô∏è OPENAI_API_KEY n√£o encontrada, pulando transcri√ß√£o")
                return {'text': '', 'segments': []}
            
            # Configurar cliente OpenAI
            client = openai.OpenAI(api_key=openai_api_key)
            
            # Chamar API Whisper com configura√ß√µes otimizadas
            with open(audio_path, 'rb') as audio_file:
                response = client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    response_format="verbose_json",
                    timestamp_granularities=["segment"],
                    language="pt",  # For√ßar portugu√™s para reduzir alucina√ß√µes
                    prompt="Gameplay de videogame. Pode conter falas do jogador ou sons do jogo. Se n√£o h√° fala humana clara, retorne texto vazio.",  # Prompt gen√©rico
                    temperature=0.0  # Temperatura zero para reduzir criatividade/alucina√ß√£o
                )
            
            # Limpar arquivo tempor√°rio
            audio_path.unlink()
            
            print(f"   ‚úÖ Transcri√ß√£o: '{response.text[:50]}...'")
            
            # Converter segments para formato JSON serializ√°vel e analisar qualidade
            segments_data = []
            high_confidence_segments = 0
            total_speech_duration = 0.0
            
            if hasattr(response, 'segments') and response.segments:
                for segment in response.segments:
                    no_speech_prob = getattr(segment, 'no_speech_prob', 1.0)
                    avg_logprob = getattr(segment, 'avg_logprob', -10.0)
                    segment_text = getattr(segment, 'text', '').strip()
                    
                    segment_dict = {
                        'id': getattr(segment, 'id', 0),
                        'start': getattr(segment, 'start', 0.0),
                        'end': getattr(segment, 'end', 0.0),
                        'text': segment_text,
                        'tokens': getattr(segment, 'tokens', []),
                        'temperature': getattr(segment, 'temperature', 0.0),
                        'avg_logprob': avg_logprob,
                        'compression_ratio': getattr(segment, 'compression_ratio', 0.0),
                        'no_speech_prob': no_speech_prob
                    }
                    segments_data.append(segment_dict)
                    
                    # Analisar qualidade da transcri√ß√£o
                    if no_speech_prob < 0.6 and avg_logprob > -3.0 and len(segment_text) > 3:
                        high_confidence_segments += 1
                        total_speech_duration += segment_dict['end'] - segment_dict['start']
            
            # Filtrar texto se qualidade for baixa (poss√≠vel alucina√ß√£o)
            filtered_text = response.text
            is_likely_hallucination = False
            
            # Crit√©rios para detectar alucina√ß√£o:
            # 1. Muitos segmentos com alta probabilidade de n√£o-fala
            # 2. Texto muito longo vs dura√ß√£o real de fala
            # 3. Palavras estranhas ou texto multil√≠ngue suspeito
            if len(segments_data) > 0:
                avg_no_speech_prob = sum(s['no_speech_prob'] for s in segments_data) / len(segments_data)
                
                suspicious_patterns = [
                    'thanks for watching', 'subscribe', 'like and subscribe',
                    'youtube', 'channel', 'click here', 'description below',
                    'video tutorial', 'follow me on', 'plug-in', 'software',
                    'alpha', 'beta', 'version', 'update available'
                ]
                
                has_suspicious_content = any(pattern.lower() in filtered_text.lower() for pattern in suspicious_patterns)
                
                if (avg_no_speech_prob > 0.7 or 
                    high_confidence_segments == 0 or 
                    has_suspicious_content or
                    len(filtered_text) > total_speech_duration * 20):  # Mais de 20 chars por segundo √© suspeito
                    
                    is_likely_hallucination = True
                    filtered_text = ""  # Limpar texto suspeito
                    print(f"   ‚ö†Ô∏è Poss√≠vel alucina√ß√£o detectada - limpando transcri√ß√£o")
                    print(f"   üìä Avg no_speech_prob: {avg_no_speech_prob:.2f}, Segmentos confi√°veis: {high_confidence_segments}")
            
            return {
                'text': filtered_text,
                'segments': segments_data,
                'language': getattr(response, 'language', 'pt'),
                'quality_analysis': {
                    'high_confidence_segments': high_confidence_segments,
                    'total_segments': len(segments_data),
                    'total_speech_duration': total_speech_duration,
                    'is_likely_hallucination': is_likely_hallucination,
                    'avg_no_speech_prob': avg_no_speech_prob if segments_data else 1.0
                }
            }
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erro na transcri√ß√£o: {e}")
            return {'text': '', 'segments': []}
    
    def _parse_volume_analysis(self, ffmpeg_output: str) -> Dict[str, float]:
        """Parseia an√°lise de volume do FFmpeg"""
        volume_info = {}
        for line in ffmpeg_output.split('\n'):
            if 'mean_volume:' in line:
                try:
                    volume_info['mean_volume'] = float(line.split('mean_volume: ')[1].split(' dB')[0])
                except:
                    pass
            elif 'max_volume:' in line:
                try:
                    volume_info['max_volume'] = float(line.split('max_volume: ')[1].split(' dB')[0])
                except:
                    pass
        return volume_info
    
    def _create_cleaning_strategy_with_ai(self, chunk_path: str, technical_analysis: Dict[str, Any], instructions: Dict[str, Any]) -> Dict[str, Any]:
        """Usa IA para criar estrat√©gia de limpeza baseada nas instru√ß√µes espec√≠ficas"""
        
        gemini_api_key = os.getenv('GEMINI_API_KEY')
        if not gemini_api_key:
            raise Exception("‚ùå GEMINI_API_KEY obrigat√≥ria!")
        
        # Extrair dados de transcri√ß√£o
        transcription = technical_analysis.get('transcription', {})
        transcript_text = transcription.get('text', '')
        quality_analysis = transcription.get('quality_analysis', {})
        
        ai_prompt = f"""Voc√™ √© DAVID, especialista em limpeza de √°udio/v√≠deo para gaming/esports.

INSTRU√á√ÉO ESPEC√çFICA DO COORDINATOR:
{json.dumps(instructions, indent=2, ensure_ascii=False)}

AN√ÅLISE T√âCNICA DO CHUNK "{Path(chunk_path).name}":
- Dura√ß√£o: {technical_analysis.get('duration', 0):.1f} segundos
- Per√≠odos de sil√™ncio: {technical_analysis.get('silence_count', 0)}
- Dura√ß√£o total sil√™ncio: {technical_analysis.get('total_silence_duration', 0):.1f}s
- Volume m√©dio: {technical_analysis.get('volume_info', {}).get('mean_volume', 'N/A')} dB
- Volume m√°ximo: {technical_analysis.get('volume_info', {}).get('max_volume', 'N/A')} dB

AN√ÅLISE DE TRANSCRI√á√ÉO:
- Texto transcrito: "{transcript_text}"
- Segmentos de alta confian√ßa: {quality_analysis.get('high_confidence_segments', 0)}/{quality_analysis.get('total_segments', 0)}
- Dura√ß√£o real de fala: {quality_analysis.get('total_speech_duration', 0):.1f}s
- Probabilidade m√©dia de n√£o-fala: {quality_analysis.get('avg_no_speech_prob', 1.0):.2f}
- Poss√≠vel alucina√ß√£o detectada: {quality_analysis.get('is_likely_hallucination', False)}

AN√ÅLISE INTELIGENTE DO CONTE√öDO:
Use as m√©tricas de qualidade da transcri√ß√£o para determinar o TIPO DE √ÅUDIO:

CRIT√âRIOS PARA GAMEPLAY S√ì COM JOGO:
- Transcri√ß√£o vazia OU poss√≠vel alucina√ß√£o detectada = TRUE
- Segmentos de alta confian√ßa = 0 ou muito poucos
- Probabilidade m√©dia de n√£o-fala > 0.6
- Texto suspeito (frases estranhas, multil√≠ngue, t√©cnicas)
- Dura√ß√£o real de fala < 10% da dura√ß√£o total do chunk
- Sons t√≠picos: Windows, cliques, m√∫sica do jogo, vozes de NPCs

CRIT√âRIOS PARA GAMEPLAY COM NARRA√á√ÉO:
- Segmentos de alta confian√ßa > 30% dos segmentos totais
- Probabilidade m√©dia de n√£o-fala < 0.5
- Dura√ß√£o real de fala > 20% da dura√ß√£o total
- Texto cont√©m coment√°rios pessoais: "nossa", "cara", "mano"
- Rea√ß√µes ao gameplay: "que tiro", "olha isso"

CRIT√âRIOS PARA CONVERSA/CHAT:
- Dura√ß√£o real de fala > 50% da dura√ß√£o total
- M√∫ltiplas vozes detectadas
- Conversas cont√≠nuas entre pessoas

REGRAS DE DECIS√ÉO BASEADAS NAS M√âTRICAS:

1. SE poss√≠vel_alucina√ß√£o = TRUE ‚Üí TIPO = "gameplay_only"
2. SE segmentos_alta_confian√ßa = 0 ‚Üí TIPO = "gameplay_only"  
3. SE prob_m√©dia_n√£o_fala > 0.7 ‚Üí TIPO = "gameplay_only"
4. SE dura√ß√£o_real_fala < 5 segundos ‚Üí TIPO = "gameplay_only"
5. SE texto_vazio OU texto_suspeito ‚Üí TIPO = "gameplay_only"

IMPORTANTE: Seja CONSERVADOR. Na d√∫vida, classifique como "gameplay_only" para preservar a experi√™ncia.

Com base nesta an√°lise BASEADA EM DADOS, crie uma estrat√©gia PRECISA.

Retorne APENAS um JSON v√°lido sem coment√°rios ou texto extra. Use strings curtas e simples:
{{
    "requires_processing": false,
    "audio_type": "gameplay_only",
    "strategy_explanation": "Gameplay puro detectado. Preservar audio original.",
    "detected_issues": {{
        "silence_periods": {technical_analysis.get('silence_count', 0)},
        "filler_words": [],
        "stutters": [],
        "hesitations": [],
        "game_audio_detected": true
    }},
    "processing_strategy": {{
        "preserve_game_audio": true,
        "cut_silence": false,
        "remove_filler_words": false,
        "fix_stutters": false,
        "audio_enhancement": true
    }},
    "audio_filters": [
        "highpass=f=80",
        "lowpass=f=15000"
    ],
    "silence_removal": {{
        "enabled": false,
        "threshold_db": -40,
        "min_duration": 0.5,
        "aggressiveness": "low"
    }},
    "volume_adjustment": {{
        "enabled": true,
        "normalize": true,
        "target_loudness": -23
    }},
    "preserve_action_audio": true,
    "intensity_level": "medium",
    "expected_improvement": "Melhoria na qualidade do audio do jogo"
}}

REGRAS OBRIGAT√ìRIAS:
0. Use APENAS caracteres ASCII simples no JSON. Evite acentos, aspas curvas, retic√™ncias especiais
1. Siga exatamente a instru√ß√£o: "{instructions.get('focus', '')}"
2. Preserve_action_audio: {instructions.get('preserve_action_audio', True)}
3. Intensity_level: {instructions.get('intensity_level', 'medium')}

DETEC√á√ÉO DE TIPO DE √ÅUDIO:
4. Se transcri√ß√£o for vazia ou s√≥ conter falas do jogo ‚Üí audio_type: "gameplay_only"
5. Se detectar jogador falando sobre gameplay ‚Üí audio_type: "gameplay_with_narration"  
6. Se detectar m√∫ltiplas pessoas conversando ‚Üí audio_type: "conversation"

REGRAS POR TIPO:
7. GAMEPLAY_ONLY: requires_processing = false (preserve tudo)
8. GAMEPLAY_WITH_NARRATION: processar apenas v√≠cios do jogador
9. CONVERSATION: cuidado com cortes que quebrem di√°logo

PRIORIDADES:
10. NUNCA corte falas importantes do jogo (callouts, comunica√ß√£o de equipe)
11. PRESERVE sons de ambiente, m√∫sica, efeitos sonoros
12. ANALISE CONTEXTO: "Fire in the hole" √© do jogo, "nossa que tiro" √© do jogador
13. Se em d√∫vida sobre quem est√° falando ‚Üí N√ÉO CORTE

Seja CONSERVADOR - melhor preservar demais que cortar demais."""

        try:
            url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={gemini_api_key}"
            
            headers = {'Content-Type': 'application/json'}
            
            payload = {
                "contents": [{"parts": [{"text": ai_prompt}]}],
                "generationConfig": {
                    "temperature": 0.3,
                    "topK": 1,
                    "topP": 1,
                    "maxOutputTokens": 800
                }
            }
            
            print(f"ü§ñ Enviando an√°lise para IA...")
            response = requests.post(url, headers=headers, json=payload, timeout=30)
            
            if response.status_code != 200:
                raise Exception(f"Erro API: {response.status_code}")
            
            result = response.json()
            ai_response = result['candidates'][0]['content']['parts'][0]['text']
            
            # Parsear JSON com tratamento robusto
            clean_response = ai_response.strip()
            
            # Remover markdown code blocks
            if clean_response.startswith('```json'):
                clean_response = clean_response[7:]
            elif clean_response.startswith('```'):
                clean_response = clean_response[3:]
            if clean_response.endswith('```'):
                clean_response = clean_response[:-3]
            
            # Limpar texto antes e depois do JSON
            clean_response = clean_response.strip()
            
            # Encontrar o JSON no texto
            start_idx = clean_response.find('{')
            end_idx = clean_response.rfind('}')
            
            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                clean_response = clean_response[start_idx:end_idx + 1]
            
            # Corrigir aspas simples para duplas (mas n√£o dentro de strings)
            # Usar regex mais cuidadoso ou m√©todo manual
            clean_response = clean_response.replace("'", '"')
            
            # Corrigir caracteres problem√°ticos que podem aparecer
            clean_response = clean_response.replace('‚Ä¶', '...')
            clean_response = clean_response.replace('"', '"').replace('"', '"')
            clean_response = clean_response.replace(''', "'").replace(''', "'")
            
            # Debug: mostrar o JSON que estamos tentando parsear
            print(f"   üîß JSON para parsear: {clean_response[:200]}...")
            
            try:
                strategy = json.loads(clean_response)
            except json.JSONDecodeError as parse_error:
                print(f"   ‚ùå JSON inv√°lido: {parse_error}")
                print(f"   üìù Linha problem√°tica: {clean_response.split(chr(10))[parse_error.lineno-1] if hasattr(parse_error, 'lineno') else 'N/A'}")
                print(f"   üìÑ Resposta completa limpa:\n{clean_response}")
                
                # Tentar fallback: remover caracteres n√£o-ASCII
                try:
                    clean_ascii = clean_response.encode('ascii', 'ignore').decode('ascii')
                    strategy = json.loads(clean_ascii)
                    print(f"   ‚úÖ Fallback ASCII funcionou!")
                except:
                    raise Exception(f"IA retornou JSON inv√°lido mesmo com fallback: {parse_error}")
            
            print(f"‚úÖ Estrat√©gia IA criada: {strategy.get('strategy_explanation', '')[:50]}...")
            
            # Salvar an√°lise da IA em arquivo JSON
            self._save_ai_analysis(chunk_path, technical_analysis, instructions, strategy)
            
            return strategy
            
        except Exception as e:
            print(f"‚ùå Erro na estrat√©gia IA: {e}")
            # Fallback para estrat√©gia b√°sica
            return {
                'requires_processing': True,
                'strategy_explanation': 'Estrat√©gia b√°sica por falha da IA',
                'audio_filters': ['highpass=f=80', 'lowpass=f=15000', 'dynaudnorm'],
                'silence_removal': {'enabled': True, 'threshold_db': -40, 'min_duration': 0.5},
                'volume_adjustment': {'enabled': True, 'normalize': True},
                'preserve_action_audio': instructions.get('preserve_action_audio', True),
                'intensity_level': instructions.get('intensity_level', 'medium')
            }
    
    def _save_ai_analysis(self, chunk_path: str, technical_analysis: Dict[str, Any], instructions: Dict[str, Any], ai_strategy: Dict[str, Any]) -> str:
        """Salva an√°lise completa da IA em arquivo JSON"""
        try:
            chunk_name = Path(chunk_path).stem
            analysis_file = self.david_dir / f"david_analysis_{chunk_name}.json"
            
            complete_analysis = {
                'agent': self.name,
                'chunk_file': Path(chunk_path).name,
                'analysis_timestamp': time.time(),
                'coordinator_instructions': instructions,
                'technical_analysis': {
                    'duration': technical_analysis.get('duration', 0),
                    'silence_periods_count': technical_analysis.get('silence_count', 0),
                    'total_silence_duration': technical_analysis.get('total_silence_duration', 0),
                    'volume_info': technical_analysis.get('volume_info', {}),
                    'transcription': technical_analysis.get('transcription', {})
                },
                'ai_strategy': ai_strategy,
                'processing_decision': {
                    'requires_processing': ai_strategy.get('requires_processing', False),
                    'audio_type_detected': ai_strategy.get('audio_type', 'unknown'),
                    'reasoning': ai_strategy.get('strategy_explanation', ''),
                    'detected_issues': ai_strategy.get('detected_issues', {}),
                    'processing_strategy': ai_strategy.get('processing_strategy', {})
                }
            }
            
            with open(analysis_file, 'w', encoding='utf-8') as f:
                json.dump(complete_analysis, f, indent=2, ensure_ascii=False)
            
            print(f"   üíæ An√°lise IA salva: {analysis_file.name}")
            return str(analysis_file)
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erro ao salvar an√°lise: {e}")
            return ""
    
    def _save_chunk_processing_result(self, chunk_filename: str, chunk_result: Dict[str, Any], analysis: Dict[str, Any]) -> str:
        """Salva resultado detalhado do processamento de cada chunk"""
        try:
            chunk_name = Path(chunk_filename).stem
            result_file = self.david_dir / f"david_processing_{chunk_name}.json"
            
            processing_result = {
                'agent': self.name,
                'chunk_file': chunk_filename,
                'processing_timestamp': time.time(),
                'processing_result': chunk_result,
                'detailed_analysis': {
                    'technical_analysis': analysis.get('technical_analysis', {}),
                    'ai_strategy': analysis.get('ai_strategy', {}),
                    'processing_decision': {
                        'needs_cleaning': analysis.get('needs_cleaning', False),
                        'audio_type_detected': analysis.get('ai_strategy', {}).get('audio_type', 'unknown'),
                        'strategy_explanation': analysis.get('ai_strategy', {}).get('strategy_explanation', ''),
                        'detected_issues': analysis.get('ai_strategy', {}).get('detected_issues', {}),
                        'processing_strategy': analysis.get('ai_strategy', {}).get('processing_strategy', {}),
                        'expected_improvement': analysis.get('ai_strategy', {}).get('expected_improvement', '')
                    }
                },
                'performance_metrics': {
                    'time_saved': chunk_result.get('time_saved', 0),
                    'processing_successful': chunk_result.get('status') == 'AI_PROCESSED',
                    'audio_filters_applied': len(analysis.get('ai_strategy', {}).get('audio_filters', [])),
                    'issues_detected': len(analysis.get('ai_strategy', {}).get('detected_issues', {}).get('filler_words', [])) + len(analysis.get('ai_strategy', {}).get('detected_issues', {}).get('stutters', []))
                },
                'transcription_data': analysis.get('technical_analysis', {}).get('transcription', {}),
                'coordinator_context': analysis.get('coordinator_instructions', {})
            }
            
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(processing_result, f, indent=2, ensure_ascii=False)
            
            print(f"   üìä Resultado do processamento salvo: {result_file.name}")
            return str(result_file)
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erro ao salvar resultado do processamento: {e}")
            return ""
    
    def _parse_silence_detection(self, ffmpeg_output: str) -> List[Dict[str, float]]:
        """Parseia output do silencedetect do FFmpeg"""
        silence_periods = []
        lines = ffmpeg_output.split('\n')
        
        current_silence = {}
        
        for line in lines:
            if 'silence_start:' in line:
                try:
                    start_time = float(line.split('silence_start: ')[1].split()[0])
                    current_silence = {'start': start_time}
                except (IndexError, ValueError):
                    continue
                    
            elif 'silence_end:' in line and current_silence:
                try:
                    end_time = float(line.split('silence_end: ')[1].split()[0])
                    duration = float(line.split('silence_duration: ')[1].split()[0])
                    
                    current_silence.update({
                        'end': end_time,
                        'duration': duration
                    })
                    
                    silence_periods.append(current_silence)
                    current_silence = {}
                    
                except (IndexError, ValueError):
                    continue
        
        return silence_periods
    
    def create_ai_audio_filter(self, ai_strategy: Dict[str, Any]) -> str:
        """Cria filtro FFmpeg baseado na estrat√©gia da IA"""
        filters = []
        
        # Usar filtros espec√≠ficos da IA
        ai_filters = ai_strategy.get('audio_filters', [])
        filters.extend(ai_filters)
        
        # Configurar remo√ß√£o de sil√™ncio baseada na IA
        silence_config = ai_strategy.get('silence_removal', {})
        if silence_config.get('enabled', False):
            threshold_db = silence_config.get('threshold_db', -40)
            min_duration = silence_config.get('min_duration', 0.5)
            aggressiveness = silence_config.get('aggressiveness', 'medium')
            
            # Ajustar par√¢metros baseado na agressividade
            if aggressiveness == 'high':
                threshold_db = max(threshold_db, -35)  # Mais sens√≠vel
                min_duration = max(min_duration * 0.7, 0.3)  # Cortes menores
            elif aggressiveness == 'low':
                threshold_db = min(threshold_db, -45)  # Menos sens√≠vel  
                min_duration = min_duration * 1.3  # Cortes maiores
            
            # IMPORTANTE: silenceremove remove √°udio E v√≠deo juntos
            # Remove in√≠cio silencioso e m√∫ltiplos segmentos de sil√™ncio
            silence_filter = f'silenceremove=start_periods=1:start_silence={min_duration}:start_threshold={threshold_db}dB:stop_periods=-1:stop_silence={min_duration}:stop_threshold={threshold_db}dB'
            filters.append(silence_filter)
        
        # Configurar ajuste de volume baseado na IA
        volume_config = ai_strategy.get('volume_adjustment', {})
        if volume_config.get('enabled', False):
            if volume_config.get('normalize', False):
                target_loudness = volume_config.get('target_loudness', -23)
                filters.append(f'loudnorm=I={target_loudness}:LRA=11:TP=-1.5')
            else:
                filters.append('dynaudnorm=p=0.9:s=1')
        
        # Se n√£o h√° filtros, usar b√°sico
        if not filters:
            filters = ['highpass=f=80', 'lowpass=f=15000', 'dynaudnorm']
        
        print(f"üéõÔ∏è Filtros IA aplicados: {len(filters)} filtros")
        return ','.join(filters)
    
    def clean_audio_chunk_with_ai(self, chunk_path: str, analysis: Dict[str, Any]) -> Optional[str]:
        """Limpa √°udio E corta v√≠deo usando estrat√©gia criada pela IA"""
        print(f"üßπ {self.name}: Processando chunk com estrat√©gia IA - {Path(chunk_path).name}")
        
        ai_strategy = analysis.get('ai_strategy', {})
        technical_analysis = analysis.get('technical_analysis', {})
        
        if not analysis.get('needs_cleaning') or not ai_strategy.get('requires_processing', True):
            audio_type = ai_strategy.get('audio_type', 'unknown')
            print(f"   ‚úÖ IA determinou: chunk n√£o precisa de processamento")
            print(f"   üéÆ Tipo de √°udio: {audio_type}")
            print(f"   üí° Motivo: {ai_strategy.get('strategy_explanation', 'N/A')}")
            
            # Se for gameplay_only, garantir que n√£o processamos
            if audio_type == 'gameplay_only':
                print(f"   üéØ Gameplay puro detectado - preservando experi√™ncia completa do jogo")
            
            return chunk_path
        
        # Criar nome do arquivo limpo
        chunk_file = Path(chunk_path)
        cleaned_filename = f"{chunk_file.stem}_cleaned_ai{chunk_file.suffix}"
        cleaned_path = self.temp_dir / cleaned_filename
        
        try:
            audio_type = ai_strategy.get('audio_type', 'unknown')
            processing_strategy = ai_strategy.get('processing_strategy', {})
            
            print(f"   üéØ Estrat√©gia: {ai_strategy.get('strategy_explanation', '')[:80]}...")
            print(f"   üéÆ Tipo de √°udio: {audio_type}")
            print(f"   üéõÔ∏è Intensidade: {ai_strategy.get('intensity_level', 'medium')}")
            print(f"   üéµ Preservar √°udio do jogo: {processing_strategy.get('preserve_game_audio', True)}")
            
            # Verificar se precisa cortar partes silenciosas
            silence_periods = technical_analysis.get('silence_periods', [])
            
            # L√≥gica baseada no tipo de √°udio
            if audio_type == 'gameplay_only':
                print(f"   üéÆ Gameplay puro - aplicando apenas melhorias b√°sicas de √°udio")
                return self._apply_audio_filters_only(chunk_path, ai_strategy, cleaned_path)
            
            elif processing_strategy.get('cut_silence') and len(silence_periods) > 0:
                print(f"   ‚úÇÔ∏è Cortando {len(silence_periods)} per√≠odos de sil√™ncio do v√≠deo...")
                return self._cut_silence_from_video(chunk_path, silence_periods, ai_strategy, cleaned_path)
            
            else:
                print(f"   üéöÔ∏è Aplicando filtros de √°udio sem cortes...")
                return self._apply_audio_filters_only(chunk_path, ai_strategy, cleaned_path)
                
        except Exception as e:
            print(f"   ‚ùå Erro no processamento: {e}")
            return None
    
    def _cut_silence_from_video(self, chunk_path: str, silence_periods: List[Dict], ai_strategy: Dict, output_path: Path) -> Optional[str]:
        """Corta partes silenciosas do v√≠deo (√°udio + v√≠deo)"""
        try:
            # Criar segmentos sem sil√™ncio
            segments_file = self.temp_dir / f"segments_{Path(chunk_path).stem}.txt"
            
            # Gerar lista de segmentos a manter
            duration = self._get_video_duration(chunk_path)
            segments = self._generate_segments_without_silence(duration, silence_periods, ai_strategy)
            
            if len(segments) == 0:
                print(f"   ‚ö†Ô∏è Nenhum segmento v√°lido encontrado")
                return chunk_path
            
            # Criar segmentos individuais primeiro
            segment_files = []
            print(f"   üìê Criando {len(segments)} segmentos...")
            
            for i, (start, end) in enumerate(segments):
                segment_file = self.temp_dir / f"segment_{i}_{Path(chunk_path).stem}.mp4"
                print(f"   üîß Segmento {i+1}/{len(segments)}: {start:.1f}s ‚Üí {end:.1f}s")
                
                # Extrair cada segmento
                cmd = [
                    'ffmpeg', '-i', chunk_path,
                    '-ss', str(start),
                    '-t', str(end - start),
                    '-c:v', 'libx264', '-c:a', 'aac',
                    '-b:a', '128k', '-crf', '23',
                    '-avoid_negative_ts', 'make_zero',
                    '-y', str(segment_file)
                ]
                
                result = subprocess.run(cmd, capture_output=True, text=True)
                
                if result.returncode == 0 and segment_file.exists():
                    segment_files.append(str(segment_file))
                    print(f"     ‚úÖ Segmento criado: {segment_file.name}")
                else:
                    print(f"     ‚ùå Erro ao criar segmento: {result.stderr}")
            
            if not segment_files:
                print(f"   ‚ùå Nenhum segmento criado")
                return None
            
            # Criar arquivo de lista para concatena√ß√£o (usar caminhos absolutos)
            with open(segments_file, 'w') as f:
                for segment_file in segment_files:
                    abs_path = Path(segment_file).resolve()
                    f.write(f"file '{abs_path}'\n")
            
            # Debug: mostrar conte√∫do do arquivo
            print(f"   üìÑ Arquivo de lista criado: {segments_file}")
            with open(segments_file, 'r') as f:
                content = f.read()
                print(f"   üìù Conte√∫do:\n{content}")
            
            # Verificar se todos os arquivos existem
            for segment_file in segment_files:
                if not Path(segment_file).exists():
                    print(f"   ‚ö†Ô∏è Arquivo n√£o existe: {segment_file}")
                else:
                    print(f"   ‚úÖ Arquivo existe: {Path(segment_file).name}")
            
            # Usar FFmpeg para concatenar segmentos
            cmd = [
                'ffmpeg', '-f', 'concat', '-safe', '0',
                '-i', str(segments_file),
                '-c', 'copy',  # Copy streams para evitar re-encoding
                '-y', str(output_path)
            ]
            
            print(f"   üîß Concatenando {len(segment_files)} segmentos...")
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode != 0:
                print(f"   ‚ùå Erro na concatena√ß√£o: {result.stderr}")
                # Tentar m√©todo alternativo com re-encoding
                cmd_alt = [
                    'ffmpeg', '-f', 'concat', '-safe', '0',
                    '-i', str(segments_file),
                    '-c:v', 'libx264', '-c:a', 'aac',
                    '-b:a', '128k', '-crf', '23',
                    '-y', str(output_path)
                ]
                print(f"   üîÑ Tentando concatena√ß√£o com re-encoding...")
                result = subprocess.run(cmd_alt, capture_output=True, text=True)
            
            # Limpar arquivos tempor√°rios
            segments_file.unlink()
            for segment_file in segment_files:
                Path(segment_file).unlink(missing_ok=True)
            
            if output_path.exists() and result.returncode == 0:
                original_duration = self._get_video_duration(chunk_path)
                new_duration = self._get_video_duration(str(output_path))
                time_saved = original_duration - new_duration
                
                print(f"   ‚úÖ Corte conclu√≠do!")
                print(f"   üìä Tempo removido: {time_saved:.1f}s ({time_saved/original_duration*100:.1f}%)")
                print(f"   üé¨ Nova dura√ß√£o: {new_duration:.1f}s")
                
                return str(output_path)
            else:
                print(f"   ‚ùå Falha na concatena√ß√£o: {result.stderr}")
                return None
                
        except subprocess.CalledProcessError as e:
            print(f"   ‚ùå Erro FFmpeg no corte: {e}")
            return None
    
    def _apply_audio_filters_only(self, chunk_path: str, ai_strategy: Dict, output_path: Path) -> Optional[str]:
        """Aplica apenas filtros de √°udio sem cortar v√≠deo"""
        try:
            audio_filter = self.create_ai_audio_filter(ai_strategy)
            
            cmd = [
                'ffmpeg', '-i', chunk_path,
                '-af', audio_filter,
                '-c:v', 'copy',  # Manter v√≠deo original
                '-c:a', 'aac',   # Re-encode √°udio
                '-b:a', '128k',  # Bitrate √°udio
                '-y', str(output_path)
            ]
            
            print(f"   üîß Aplicando {len(audio_filter.split(','))} filtros de √°udio...")
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            
            if output_path.exists():
                print(f"   ‚úÖ Filtros aplicados com sucesso!")
                return str(output_path)
            else:
                return None
                
        except subprocess.CalledProcessError as e:
            print(f"   ‚ùå Erro FFmpeg nos filtros: {e}")
            return None
    
    def _generate_segments_without_silence(self, duration: float, silence_periods: List[Dict], ai_strategy: Dict) -> List[tuple]:
        """Gera lista de segmentos de v√≠deo removendo sil√™ncios"""
        segments = []
        current_time = 0.0
        
        # Configura√ß√µes baseadas na estrat√©gia IA
        silence_config = ai_strategy.get('silence_removal', {})
        min_segment_duration = 1.0  # M√≠nimo 1 segundo por segmento
        
        for silence in silence_periods:
            silence_start = silence['start']
            silence_end = silence['end']
            
            # Adicionar segmento antes do sil√™ncio
            if silence_start > current_time:
                segment_duration = silence_start - current_time
                if segment_duration >= min_segment_duration:
                    segments.append((current_time, silence_start))
            
            # Pular o sil√™ncio
            current_time = silence_end
        
        # Adicionar segmento final ap√≥s √∫ltimo sil√™ncio
        if current_time < duration:
            segment_duration = duration - current_time
            if segment_duration >= min_segment_duration:
                segments.append((current_time, duration))
        
        print(f"   üìê Segmentos gerados: {len(segments)} de {duration:.1f}s total")
        return segments
    
    def _get_video_duration(self, video_path: str) -> float:
        """Obt√©m dura√ß√£o do v√≠deo em segundos"""
        try:
            cmd = [
                'ffprobe', '-v', 'quiet', '-print_format', 'json',
                '-show_format', video_path
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            info = json.loads(result.stdout)
            
            return float(info['format']['duration'])
            
        except Exception:
            return 0.0
    
    def process_chunks_with_ai(self, processing_plan: Dict[str, Any]) -> Dict[str, Any]:
        """Processa chunks usando IA para estrat√©gias personalizadas"""
        print(f"\nü§ñ {self.name}: Iniciando processamento com IA...")
        
        david_instructions = processing_plan['strategy']['agent_instructions']['DAVID']
        chunks_to_process = david_instructions['chunks_to_process']
        
        results = {
            'agent': self.name,
            'processed_chunks': [],
            'ai_strategies_used': [],
            'total_time_saved': 0.0,
            'processing_time': time.time(),
            'status': 'IN_PROGRESS',
            'coordinator_instructions': david_instructions
        }
        
        print(f"üìã Instru√ß√µes do Coordinator:")
        print(f"   üéØ Foco: {david_instructions.get('focus', 'N/A')}")
        print(f"   üéµ Preservar √°udio de a√ß√£o: {david_instructions.get('preserve_action_audio', True)}")
        print(f"   üéõÔ∏è Intensidade: {david_instructions.get('intensity_level', 'medium')}")
        print(f"üìä Chunks para processar: {len(chunks_to_process)}")
        
        for i, chunk_filename in enumerate(chunks_to_process, 1):
            chunk_path = self.chunks_dir / chunk_filename
            
            if not chunk_path.exists():
                print(f"‚ö†Ô∏è Chunk {i}/{len(chunks_to_process)}: {chunk_filename} n√£o encontrado")
                continue
            
            print(f"\n{'='*60}")
            print(f"üé¨ Processando chunk {i}/{len(chunks_to_process)}: {chunk_filename}")
            print(f"{'='*60}")
            
            try:
                # Analisar chunk com IA
                analysis = self.analyze_chunk_with_ai(str(chunk_path), david_instructions)
                
                # Limpar √°udio com estrat√©gia IA
                cleaned_path = self.clean_audio_chunk_with_ai(str(chunk_path), analysis)
                
                if cleaned_path:
                    ai_strategy = analysis.get('ai_strategy', {})
                    technical_analysis = analysis.get('technical_analysis', {})
                    
                    # Calcular tempo salvo corretamente baseado no processamento
                    requires_processing = ai_strategy.get('requires_processing', True)
                    actual_time_saved = 0.0
                    
                    if requires_processing and cleaned_path != str(chunk_path):
                        # S√≥ conta tempo salvo se realmente processou e gerou arquivo diferente
                        actual_time_saved = technical_analysis.get('total_silence_duration', 0)
                    
                    chunk_result = {
                        'original_chunk': chunk_filename,
                        'cleaned_chunk': Path(cleaned_path).name,
                        'time_saved': actual_time_saved,
                        'ai_strategy_used': ai_strategy.get('strategy_explanation', ''),
                        'audio_type_detected': ai_strategy.get('audio_type', 'unknown'),
                        'requires_processing': requires_processing,
                        'expected_improvement': ai_strategy.get('expected_improvement', ''),
                        'processing_strategy': ai_strategy.get('processing_strategy', {}),
                        'detected_issues': ai_strategy.get('detected_issues', {}),
                        'status': 'AI_PROCESSED' if requires_processing else 'PRESERVED'
                    }
                    
                    results['processed_chunks'].append(chunk_result)
                    results['total_time_saved'] += chunk_result['time_saved']
                    
                    # Armazenar estrat√©gia √∫nica para relat√≥rio
                    strategy_summary = {
                        'chunk': chunk_filename,
                        'audio_type': ai_strategy.get('audio_type', 'unknown'),
                        'strategy': ai_strategy.get('strategy_explanation', ''),
                        'filters_used': len(ai_strategy.get('audio_filters', [])),
                        'intensity': ai_strategy.get('intensity_level', 'medium'),
                        'issues_found': len(ai_strategy.get('detected_issues', {}).get('filler_words', [])) + len(ai_strategy.get('detected_issues', {}).get('stutters', []))
                    }
                    results['ai_strategies_used'].append(strategy_summary)
                    
                    # Salvar resultado individual do chunk
                    self._save_chunk_processing_result(chunk_filename, chunk_result, analysis)
                    
                else:
                    chunk_result = {
                        'original_chunk': chunk_filename,
                        'status': 'FAILED',
                        'error': 'Processamento IA falhou'
                    }
                    results['processed_chunks'].append(chunk_result)
                    
            except Exception as e:
                print(f"‚ùå Erro no processamento do chunk {chunk_filename}: {e}")
                chunk_result = {
                    'original_chunk': chunk_filename,
                    'status': 'ERROR',
                    'error': str(e)
                }
                results['processed_chunks'].append(chunk_result)
        
        results['processing_time'] = time.time() - results['processing_time']
        results['status'] = 'COMPLETED'
        
        # Relat√≥rio final
        successful_chunks = [c for c in results['processed_chunks'] if c['status'] in ['AI_PROCESSED', 'PRESERVED']]
        preserved_chunks = [c for c in results['processed_chunks'] if c['status'] == 'PRESERVED']
        processed_chunks = [c for c in results['processed_chunks'] if c['status'] == 'AI_PROCESSED']
        
        print(f"\nüéâ {self.name}: PROCESSAMENTO IA CONCLU√çDO!")
        print(f"üìä Chunks analisados com sucesso: {len(successful_chunks)}/{len(chunks_to_process)}")
        print(f"üé¨ Chunks processados (limpeza): {len(processed_chunks)}")
        print(f"üéÆ Chunks preservados (gameplay puro): {len(preserved_chunks)}")
        print(f"‚è±Ô∏è  Tempo total economizado: {results['total_time_saved']:.1f}s")
        print(f"üïê Tempo de processamento: {results['processing_time']:.1f}s")
        print(f"ü§ñ Estrat√©gias IA √∫nicas: {len(set(s['strategy'] for s in results['ai_strategies_used']))}")
        
        return results
    
    def save_results(self, results: Dict[str, Any]) -> str:
        """Salva resultados do processamento"""
        results_file = self.david_dir / f'{self.name.lower()}_results.json'
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"üìÑ Resultados salvos: {results_file}")
        return str(results_file)
    
    def execute_ai_audio_cleaning(self) -> bool:
        """M√©todo principal - executa limpeza de √°udio usando IA"""
        print(f"\n{'='*60}")
        print(f"üéµü§ñ {self.name}: INICIANDO LIMPEZA DE √ÅUDIO COM IA")
        print(f"{'='*60}")
        
        # Verificar API key
        if not os.getenv('GEMINI_API_KEY'):
            print("‚ùå GEMINI_API_KEY n√£o encontrada! Configure a API key.")
            return False
        
        # Carregar plano do Coordinator
        processing_plan = self.load_processing_plan()
        if not processing_plan:
            return False
        
        print(f"üìã Plano carregado do Coordinator")
        print(f"üéØ Tipo de conte√∫do: {processing_plan['strategy']['target_content_type']}")
        print(f"‚è±Ô∏è Dura√ß√£o alvo: {processing_plan['strategy']['target_duration']}s")
        
        # Processar chunks com IA
        results = self.process_chunks_with_ai(processing_plan)
        
        # Salvar resultados
        results_file = self.save_results(results)
        
        # Relat√≥rio final detalhado
        total_chunks = len(results['processed_chunks'])
        successful_chunks = [c for c in results['processed_chunks'] if c['status'] in ['AI_PROCESSED', 'PRESERVED']]
        preserved_chunks = [c for c in results['processed_chunks'] if c['status'] == 'PRESERVED']
        processed_chunks = [c for c in results['processed_chunks'] if c['status'] == 'AI_PROCESSED']
        
        print(f"\nüéâ {self.name}: LIMPEZA DE √ÅUDIO IA CONCLU√çDA!")
        print(f"üìã Resultados detalhados: {results_file}")
        print(f"üìä Taxa de sucesso: {len(successful_chunks)}/{total_chunks} chunks analisados")
        print(f"üé¨ Processados: {len(processed_chunks)} | üéÆ Preservados: {len(preserved_chunks)}")
        print(f"ü§ñ IA aplicou {len(results['ai_strategies_used'])} estrat√©gias personalizadas")
        print(f"üë• Pr√≥ximo agente: SAIMON (sele√ß√£o de conte√∫do)")
        
        return True
    
    def _generate_david_feedback(self, results: Dict[str, Any], processing_plan: Dict[str, Any]) -> str:
        """Gera feedback espec√≠fico do David para o Coordinator"""
        
        decisions_made = []
        problems_found = []
        recommendations = []
        
        # Analisar resultados e criar decis√µes estruturadas
        for chunk_result in results.get('processed_chunks', []):
            decision = self.create_decision_record(
                decision_type=DecisionTypes.AUDIO_PROCESSING,
                decision=f"audio_type: {chunk_result.get('audio_type_detected', 'unknown')}",
                reasoning=chunk_result.get('ai_strategy_used', 'No strategy applied'),
                confidence=0.9 if chunk_result.get('status') == 'PRESERVED' else 0.8,
                data_used={
                    'chunk': chunk_result.get('original_chunk'),
                    'requires_processing': chunk_result.get('requires_processing', False),
                    'time_saved': chunk_result.get('time_saved', 0),
                    'processing_strategy': chunk_result.get('processing_strategy', {})
                }
            )
            decisions_made.append(decision)
        
        # Identificar problemas
        failed_chunks = [c for c in results.get('processed_chunks', []) if c.get('status') in ['FAILED', 'ERROR']]
        if failed_chunks:
            problems_found.extend([f"Falha no processamento: {c['original_chunk']}" for c in failed_chunks])
        
        # Recomenda√ß√µes para pr√≥ximos agentes
        preserved_count = len([c for c in results.get('processed_chunks', []) if c.get('status') == 'PRESERVED'])
        processed_count = len([c for c in results.get('processed_chunks', []) if c.get('status') == 'AI_PROCESSED'])
        
        if preserved_count > 0:
            recommendations.append(f"Saimon: {preserved_count} chunks preservados cont√™m gameplay puro - foque em momentos visuais √©picos")
        
        if processed_count > 0:
            recommendations.append(f"Saimon: {processed_count} chunks processados t√™m √°udio limpo - podem conter narra√ß√£o importante")
        
        if results.get('total_time_saved', 0) > 0:
            recommendations.append(f"Cloe: {results['total_time_saved']:.1f}s economizados - ajustar timing para manter sincronismo")
        
        # M√©tricas de performance
        metrics = {
            'chunks_analyzed': len(results.get('processed_chunks', [])),
            'chunks_preserved': preserved_count,
            'chunks_processed': processed_count,
            'total_time_saved': results.get('total_time_saved', 0),
            'processing_time': results.get('processing_time', 0),
            'ai_strategies_used': len(results.get('ai_strategies_used', [])),
            'average_confidence': sum(d['confidence'] for d in decisions_made) / len(decisions_made) if decisions_made else 0
        }
        
        # Determinar sucesso geral
        success = (results.get('status') == 'COMPLETED' and 
                  len(failed_chunks) == 0 and 
                  len(decisions_made) > 0)
        
        return self.generate_feedback(
            success=success,
            decisions_made=decisions_made,
            problems_found=problems_found,
            recommendations=recommendations,
            metrics=metrics
        )

def main():
    """Fun√ß√£o principal do Agent David"""
    david = AgentDavid()
    
    success = david.execute()
    
    if success:
        print(f"\n‚úÖ Agent David IA conclu√≠do com sucesso!")
        print(f"üìÅ Verifique temp_audio/ para chunks processados")
        print(f"üìã Verifique chunks/david_results.json para estrat√©gias IA")
    else:
        print(f"\n‚ùå Falha no processamento do Agent David")

if __name__ == "__main__":
    main()